# HDInsight and Data Lake Storage Gen2

#### Overview

This document covers the CLI steps to create HDInsight, then registering Data Lake Storage Gen2 on it.

### Key features

* You can spin clusters with Spark, Hadoop, Kafka, HBase etc.
* Seamless integration with other components like Gen2 storage, BLOB, Data factory etc.
* Auto-scale feature available for Spark and Hive clusters. 
    You can set the cluster size based on load type or schedule type.
* Time for the cluster setup is 20 minutes.
* Also as cluster always keeps running, there is no option to shut down to keep costs down.

#### Steps


* **Prerequisite**: 
    * Make sure Data Lake Storage Gen2 is created
    * Make sure user-identity is created and is assigned the role.
    
* **Environmnent Variables**: Set the environment variables as needed

    ```
    export LOC=westus
    az account set --subscription <subscriptionid>
    export RESOURCE_GROUP=rg-casestudy1
    export IDENTITY=identity-casestudy1
    export AZ_DLAKE_GEN2=storagegen2casestudy1
    ```

* **CLI Deployment**: 

    As HDInsight keeps running all the times, you might want to create the cluster only when you need it.
    To create it using portal UI might be time consuming. So you can do the deployment using Azure CLI. 
    For this you would need two files to be created - `template.json` and `parameters.json`.
    There are two ways to create these files.
    
    * Using the portal
        
        * Go to the portal, select appropriate cluster size and 
            other things like Gen2 storage, managed identity etc.
            
        * On the last page, do not click on 'Create', rather select the option `Download template and parameters`
        
        * This would download a zip file containing `template.json`, `parameters.json` and some other files.
        
        * These files would have everything set properly as per your selection.
        
        * You would have to add the password in `parameters.json`.
     
    * Directly using JSON
        
        * Download the [template json](https://github.com/Azure-Samples/hdinsight-data-lake-storage-gen2-templates/blob/master/hdinsight-adls-gen2-template.json)
        
        * Download the [parameters.json](https://github.com/Azure-Samples/hdinsight-data-lake-storage-gen2-templates/blob/master/parameters.json)
        
        * Edit the `template.josn` and `parameters.json`
            * Change the `SUBSCRIPTION_ID`
            * Change the location to `westus`
            * Change the clusterVersion to `3.6`
            * Change clusterWorkerCount to `2`
            * Set the clusterLoginPassword and sshPassword to same value
            * Change spark to `2.3`
            * Change storageaccounts to `gen2storagecasestudy1`
            * Set resource group `rg-casestudy1`
            * Change managed identify `identity-casestudy1`
            * Change all targetInstanceCount to `2`
            * Change all vmSize to `Standard_A2m_V2` for `headnode` and `workernodes`.
            
            _Note: You can check list of nodes using the command ``._
            
    * Once `template.json` and `parameters.json` is ready, using the following command create the HDInsight Cluster.
    
        ```
        export HDINSIGHT_CLUSTER=hdinsight-casestudy1
        
        az group deployment create --name $HDINSIGHT_CLUSTER --resource-group $RESOURCE_GROUP --template-file template1/template.json --parameters template1/parameters.json
        ```
    
    * The command would run for about 10-15 minutes. The output from command would be JSON endnig as below:
    
        ```json
        {
          id": "/subscriptions/xxxxxxx/resourceGroups/rg-casestudy1/providers/Microsoft.Resources/deployments/hdinsight-casestudy1",
          "location": null,
        {
        ...
        "provisioningState": "Succeeded",
        "template": null,
        "templateHash": "4181865115933066727",
        "templateLink": null,
        "timestamp": "2019-07-13T13:24:32.059946+00:00"
      },
      "resourceGroup": "rg-casestudy1",
      "type": null
        }
        ```
    
    * In about 2-3 minutes, the cluster would be visible in the portal. 
    But it would take about 10 to 15 minutes for it to be completely operational.
    Once it is ready, the status would be should as `Operational`.
    
    * [25 mins] So your cluster is now ready to be operated.
    
    * _Note: If the deploy command fails with error like invalid storage or resource group, 
    first you have to correct the details. Then go to the portal and you would see the cluster.
    When you open it, the status would be `in error`. Delete this cluster and then rerun the CLI command._
    
* **Accessing Storage Gen2 as HDFS**

    * While deploying, you have registered Storage Gen2. 
    So the HDFS file system of the cluster would be on ABFS (Azure Blob File System).
    
    * If you open the BLOB container and connect to Gen2 account, you would see the HDFS files there.
    
    * SSH to the cluster from your machine
    
        ```
        ssh sshuser@hdinsight-casestudy1-ssh.azurehdinsight.net
        ```
    
    * Check the HDFS files, they would be the same as shown on the BLOB explorer. 
    This file system is specific to this cluster.
    
        ```
        hdfs dfs -ls /
        ```
    
    * Depending on if the BLOB container is existing or you need new one, you can take follow any of the below steps. 
        As the cluster is using managed identify, it can access the BLOB
    
        * Create New - If container does not exists, you can run the below command. 
            This step would create a container on Gen2 storage. 
        
            ```
            hadoop fs -D "fs.azure.createRemoteFileSystemDuringInitialization=true" -ls abfs://data-lake-blob@gen2storagecasestudy1.dfs.core.windows.net/
            ```
        
        * Existing - If there is existing BLOB container, you can just list them using HDFS command:
        
            ```
            hdfs dfs -ls abfs://data-lake-blob@gen2storagecasestudy1.dfs.core.windows.net/
            ```
    
    * Old way of using BLOB - you have to set the Access key in config variable `fs.azure.account.key.XYZ.blob.core.windows.net`
    
    * Create a directory on the ABFS using hdfs command.
    
        ```
        hdfs dfs -mkdir -p abfs://data-lake-blob@gen2storagecasestudy1.dfs.core.windows.net/pycharm_data/a_raw/
        ```
    
    * Refresh your blob container and it would be visible there as well.
    
    * Now you can connect to the master node and install python packages that you need to install there.
    
    * Setup PyCharm connectivity and run the project code there.
    
    
    
    
